{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jay creps + LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index \n",
    "[Kafka Architecture](#Kafka-architecture)  \n",
    "[Zookeeper](#Zookeeper)    \n",
    "[Scala Code](#Scala-code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka architecture\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/kafka architecture.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical Kafka cluster can consists of multiple brokers. \n",
    "+ They help in load balancing message reads and writes to the cluster. Each of those brokers are stateless. \n",
    "> However, they use **Zookeeper** for maintaining their states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Each topic partition has one of the brokers as leader and zero or more brokers as followers. The leader manages any read or write requests for their respective partitions. Followers replicate the leader in background without actively interfering with leader working. \n",
    "> You should think followers as a backup for leader and one of those followers would be chosen as leader in case of leader failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #6d904f;\n",
    "    border-color: green;\n",
    "    border-left: 5px solid green;\n",
    "    padding: 0.5em;\">**Producers** push data to brokers. <span>\n",
    "> At the time of publishing data, Producers search for elected leader (broker) of respective topic partition and automatically sends a message to that leader broker server. \n",
    "\n",
    "**Consumers** reads messages from brokers. \n",
    "> Consumer maintains its state with the help of Zookeepers since Kafka brokers are stateless. \n",
    "\n",
    "+ This design helps in scaling Kafka well.  Consumer offset value is maintained by Zookeeper. \n",
    "+ The consumer maintains how many messages have been consumed by it using partition offset. It ultimately acknowledges that message offset to Zookeeper. It means that the consumer has consumed all prior messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #8b8b8b;\n",
    "    border-color: black;\n",
    "    border-left: 5px solid black;\n",
    "    padding: 0.5em;\">Database analogy </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database tables are the **topics** in Kafka,         \n",
    "Applications who are inserting data into tables are **Producers** and       \n",
    "Applications who are reading data are **Consumers**.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zookeeper\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/apache_zookeeper.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka cannot work without Zookeeper. Kafka uses zookeeper for the following:\n",
    "+ __Choosing a Controller:__ The controller is one of the brokers with responsibilities of partition managements with respect to leader election, topic creation, partition creation and replica managements. When a node or server shuts down, Kafka controllers elect partition leaders from followers. Kafka uses Zookeeper's metadata information to elect a controller. Zookeeper ensures that a new controller is elected in case of current controller crashes.\n",
    "+ __Brokers Metadata:__ Zookeeper maintains the state of each of the brokers that are part of the Kafka cluster. It maintains all relevant metadata about each of the broker in a cluster. Producer/Consumer do interact with Zookeepers for getting the brokers state.\n",
    "+ __Topic Metadata:__ Zookeepers also maintain topic metadata like number of partitions, specific configuration parameters and so on.\n",
    "+ __Client Quota Information:__ With newer versions of Kafka, Quota features are introduced. Quotas enforce byte-rate thresholds on clients for reading and writing messages to Kafka topic. All these information and states are maintained by Zookeeper.\n",
    "+ __Kafka Topic ACLs:__ Kafka has an in-built authorization module which is defined as Access Control Lists (ACLs ). These ACLs determine user roles and what kind of read and write permissions each of those roles have on respective topics. Kafka uses zookeeper to store all ACLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scala code\n",
    "[Back to Index](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #f9f8aa;\n",
    "    border-color: yellow;\n",
    "    border-left: 5px solid yellow;\n",
    "    padding: 0.5em;\">**Main** takes arguments and starts producer application </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala    \n",
    "object KafkaBroker extends App {\n",
    "\n",
    "\n",
    "  case class Coordinates(lat: Double, lon: Double)\n",
    "\n",
    "  override def main(args: Array[String]): Unit = {\n",
    "\n",
    "    // parameters\n",
    "    val topic = args(0) // plume_pollution\n",
    "    val brokers = args(1) // localhost:9092 - \"broker1:port,broker2:port\"\n",
    "    val lat = args(2).toDouble // latitude - test value: 48.85\n",
    "    val lon = args(3).toDouble // longitude - test value: 2.294\n",
    "    val sleepTime = args(4).toInt // 1000 - time between queries to API\n",
    "\n",
    "    // user 'lat' and 'lon' to create Coordinates object\n",
    "    val location = Coordinates(lat, lon)\n",
    "\n",
    "    startIngestion(brokers, topic, location, sleepTime)\n",
    "\n",
    "\n",
    "  } // end of main\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #f9f8aa;\n",
    "    border-color: yellow;\n",
    "    border-left: 5px solid yellow;\n",
    "    padding: 0.5em;\">**startBroker** creates a new Kafka Producer with specified properties <span>\n",
    "\n",
    "Below are 3 mandatory configuration parameter:\n",
    "\n",
    "1. **bootstrap.servers:** This contains list of Kafka brokers address. Address is specified in terms of hostname:port. We can specify one or more broker detail, but we recommend to provide atleast 2 so if one broker goes down Producer can use other one.\n",
    "\n",
    "2. **key.serializer :** The massage is sent to Kafka brokers in the form of key value pair. Brokers expect this kay value to be in byte arrays. So we need to tell producer which serializer class to be used to convert this key value object to byte array. This property is set to tell producer that which class to use to serialize key of message. Kafka provide us 3 inbuilt serializer class ByteArraySerializer , StringSerializer and IntegerSerializer . All this classes are present under org.apache.kafka.common.serialization package and implements Serializer interface.\n",
    "\n",
    "3. **value.serializer :** Similar to key.serializer property but this property tells producer , which class to use to serialize value . You can implement your own serialize class and assign to this property.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala   \n",
    "/**\n",
    "    * Helper function to create a KafkaProducer using brokers ip and port\n",
    "    *\n",
    "    * @param brokers Broker information in the format 'localhost:9092'\n",
    "    *                or \"broker1:port,broker2:port\"\n",
    "    *\n",
    "    * @return KafkaProducer[String, String]\n",
    "    */\n",
    "\n",
    "  def startBroker(brokers:String): KafkaProducer[String, String] = {\n",
    "\n",
    "    // Kafka Broker properties\n",
    "    val props = new Properties()\n",
    "    props.put(\"bootstrap.servers\", brokers)\n",
    "    props.put(\"client.id\", \"ScalaKafkaProducer\")\n",
    "    props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(\"acks\", \"all\")\n",
    "    props.put(\"retries\", new Integer(1))\n",
    "    //    props.put(\"batch.size\", new Integer(16384))\n",
    "    //    props.put(\"linger.ms\", new Integer(1))\n",
    "    //    props.put(\"buffer.memory\", new Integer(133554432))\n",
    "\n",
    "    // TODO: implement ProducerCallback()\n",
    "\n",
    "    new KafkaProducer[String, String](props)\n",
    "\n",
    "  } // end of startBroker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"background-color: #f9f8aa;\n",
    "    border-color: yellow;\n",
    "    border-left: 5px solid yellow;\n",
    "    padding: 0.5em;\">**startIngestion** has the following objectives:</span>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load [Plume.io] token\n",
    "    + initialize **KafkaProducer**\n",
    "    + create a **ProducerRecord** by querying API with specified interval\n",
    "        + **ProducerRecord** contains a topic name, partition number, Timestamp, key and value. \n",
    "            + Optional: Partition number, Timestamp and key\n",
    "            + Mandatory: <u>topic</u> to which data will be sent and <u>value</u> which contains data are mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "    /**\n",
    "    * Queries plume pollution API for a particular 'location' (lat, long) in an interval defined by 'sleepTime'\n",
    "    * and creates a KafkaProducer to ingest content\n",
    "    *\n",
    "    * @param brokers Broker information in the format 'localhost:9092'\n",
    "    *                or \"broker1:port,broker2:port\"\n",
    "    * @param topic Topic to publish message to\n",
    "    * @param location Latitude and Longitude to query pollution\n",
    "    * @param sleepTime Time interval between queries to plume API\n",
    "    *\n",
    "    */\n",
    "\n",
    "\n",
    "  def startIngestion(brokers:String, topic:String, location: Coordinates, sleepTime: Int) = {\n",
    "\n",
    "    // access plume token https://github.com/zipfian/cartesianproduct2/wiki/TOKEN\n",
    "    lazy val token:Option[String] = sys.env.get(\"PLUMETOKEN\") orElse {\n",
    "      println(\"No token found. Check how to set it up at https://github.com/zipfian/cartesianproduct2/wiki/TOKEN\")\n",
    "      None\n",
    "    }\n",
    "\n",
    "    while (true){\n",
    "\n",
    "      // create producer with 'props' properties\n",
    "      val producer = startBroker(brokers)\n",
    "\n",
    "      // query web API - response will be a String\n",
    "      val response = Source.fromURL(\n",
    "        \"https://api.plume.io/1.0/pollution/forecast?token=\"+ token.get +\"&lat=\"+ location.lat +\"&lon=\"+ location.lon\n",
    "      ).mkString\n",
    "\n",
    "      val producerRecord = new ProducerRecord[String, String](topic, response)\n",
    "      val recordMetadata = producer.send(producerRecord)\n",
    "\n",
    "      val meta = recordMetadata.get() // I could use this to write some tests\n",
    "      val msgLog =\n",
    "        s\"\"\"\n",
    "           |topic     = ${meta.topic()}\n",
    "           |offset    = ${meta.offset()}\n",
    "           |partition = ${meta.partition()}\n",
    "          \"\"\".stripMargin\n",
    "      println(msgLog)\n",
    "\n",
    "      producer.close()\n",
    "\n",
    "      // pause in between queries - this should be an argument\n",
    "      Thread.sleep(sleepTime)\n",
    "\n",
    "    } // end of infinity loop\n",
    "\n",
    "\n",
    "  } // end of startIngestion\n",
    "\n",
    "} // end of KafkaBroker object\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka provides us Callback interface which helps in dealing with message reply irrespective of error or successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `send(ProducerRecord, new Callback())`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scala\n",
    "class ProducerCallback extends Callback {\n",
    "    \n",
    "    override def onCompletion(recordMetadata: RecordMedata, ex: Exception): = {\n",
    "        if (ex) {\n",
    "         // handle experienced exceptions   \n",
    "        }\n",
    "        else {\n",
    "            // what was done in startingestion\n",
    "            val meta = recordMetadata.get() \n",
    "            val msgLog =\n",
    "        s\"\"\"\n",
    "           |topic     = ${meta.topic()}\n",
    "           |offset    = ${meta.offset()}\n",
    "           |partition = ${meta.partition()}\n",
    "          \"\"\".stripMargin\n",
    "            println(msgLog)\n",
    "            \n",
    "        } // end of else\n",
    "    } // end of onCompletion\n",
    "} // end of ProducerCallback class\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mvnrepository.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafka Bash commands     \n",
    "Links to documentation (Kafka tutorial)     \n",
    "Link to Scala documentation      \n",
    "Link to Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
